{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfbllgrn/DD2412_Class_Contrastive_Explanations/blob/main/experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyJVkIHJo4fY"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sfbllgrn/DD2412_Class_Contrastive_Explanations.git\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_folder = '/content/DD2412_Class_Contrastive_Explanations'\n",
        "destination_folder = '/content'\n",
        "\n",
        "# List the files and subdirectories in the source folder\n",
        "contents = os.listdir(source_folder)\n",
        "\n",
        "# Move each item from the source folder to the destination folder\n",
        "for item in contents:\n",
        "    source_path = os.path.join(source_folder, item)\n",
        "    destination_path = os.path.join(destination_folder, item)\n",
        "    shutil.move(source_path, destination_path)\n",
        "\n",
        "# Remove the now-empty source folder\n",
        "os.rmdir(source_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_c_lwZYNjSAo"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torchvision.models import densenet161, DenseNet161_Weights    \n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "from torchvision.models import alexnet, AlexNet_Weights\n",
        "from torchvision.models import googlenet, GoogLeNet_Weights\n",
        "from torchvision.models import mnasnet0_5, MNASNet0_5_Weights # Här gissar jag att dom använder 0.5, står inte någonstans\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd.functional import jacobian as J\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "from torchvision import transforms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bo6tlZJkYMbA"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "\n",
        "data_folder = \"/Users/sofia/Documents/Skola/KTH/Master/Deep Learning, Advanced Course DD2412/Class Contrastive Explanations/DD2412_Class_Contrastive_Explanations/Data_small\"\n",
        "data_obj = ImageFolder(root=data_folder, transform=DenseNet161_Weights.DEFAULT.transforms())\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "val_dataloader = DataLoader(data_obj, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Init Pretrained models\n",
        "\n",
        "# densenet = densenet161(weights=DenseNet161_Weights.IMAGENET1K_V1)\n",
        "# mobilenet_small = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "alex = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
        "# google = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "# mnasnet = mnasnet0_5(weights=MNASNet0_5_Weights.IMAGENET1K_V1)\n",
        "# resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "# mobilenet_large = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)  # Denna har även IMAGENET1K_v2\n",
        "# efficientnet = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V1) # Denna har även IMAGENET1K_v2\n",
        "# pretrained_models = {\n",
        "#                     \"alexnet\":alex, \"googlenet\":google, \n",
        "#                     \"mnasnet\":mnasnet, \"resnet\":resnet, \n",
        "#                     \"mobilenet_large\":mobilenet_large, \n",
        "#                     \"efficientnet\":efficientnet,\n",
        "#                     \"densenet\":densenet, \"mobilenet_small\":mobilenet_small, }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rIKc699zb1uz"
      },
      "outputs": [],
      "source": [
        "# Perform gradient sign pertubations\n",
        "\n",
        "\n",
        "def attribution_explanation(net, x, pred_indx):\n",
        "  value_logits = J(lambda x:net(x)[np.arange(BATCH_SIZE), pred_indx], x)\n",
        "  value_logits = torch.diagonal(value_logits)\n",
        "  value_logits = value_logits.permute(3, 0, 1, 2)\n",
        "  value_probs = J(lambda x:torch.nn.functional.softmax(net(x))[np.arange(BATCH_SIZE),pred_indx], x)\n",
        "  return value_logits, value_probs\n",
        "\n",
        "\n",
        "def gradient_sign_pertube(data, net, n):\n",
        "  epsilon = 1e-3\n",
        "  x = data\n",
        "  alpha = epsilon/n\n",
        "  outputs = net(x)\n",
        "  _, predictions = torch.max(outputs, 1)\n",
        "  for i in range(n):\n",
        "    # Att tänka ut: ska det vara x eller data nedan\n",
        "    logits, probs = attribution_explanation(net, data, predictions)\n",
        "    x = x + alpha*np.sign(logits)  # blir det rätt index här?\n",
        "    x = np.clip(x, np.minimum(x-epsilon, 0), np.maximum(x+epsilon, 1))\n",
        "  return x\n",
        "\n",
        "\n",
        "class PerturbationTransform:\n",
        "    def __init__(self, network, n):\n",
        "        self.network = network\n",
        "        self.n = n\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return gradient_sign_pertube(x, self.network, self.n)\n",
        "\n",
        "\n",
        "def get_perturbed_dataloader(net):\n",
        "  perturbed_dict = {}\n",
        "  for n in [2]:#[1,2,10]:\n",
        "    perturbed_dataset = ImageFolder(root=data_folder, transform=transforms.Compose([\n",
        "    DenseNet161_Weights.DEFAULT.transforms(),\n",
        "    PerturbationTransform(net, n),  # Apply your Pertube function with the network\n",
        "    ]))\n",
        "\n",
        "    perturbed_dict[n] = DataLoader(perturbed_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "  return perturbed_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evalutate on original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "Validation Accuracy for alex with original data: 70.0\\%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on Data\n",
        "subset_size = 5\n",
        "\n",
        "pretrained_models = {\"alex\":alex}\n",
        "for name, model in pretrained_models.items():\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        subset_size = 5\n",
        "        # Loop through a subset of the validation data\n",
        "        for batch_idx, (inputs, labels) in enumerate(val_dataloader):\n",
        "            print(batch_idx)\n",
        "            if batch_idx < subset_size:\n",
        "                outputs = model(inputs)\n",
        "                # Calculate any metrics you need here\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            else:\n",
        "                break \n",
        "        \n",
        "        accuracy = correct / total\n",
        "        print('Validation Accuracy for {} with original data: {}\\%'.format(name, accuracy*100))\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Debug\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/venv/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "Validation Accuracy for alexnet with pertubed data, 10 iterations: 70.0\\%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "name = \"alexnet\"\n",
        "model = alex\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "subset_size = 5\n",
        "correct = 0\n",
        "total = 0  \n",
        "\n",
        "n=10\n",
        "with torch.no_grad():\n",
        "    correct_pert = 0\n",
        "    total_pert = 0\n",
        "    for batch_idx, (inputs, labels) in enumerate(val_dataloader):\n",
        "        #print(f(inputs, [1,2], model))\n",
        "        #sys.exit()\n",
        "        pertubed_inputs = gradient_sign_pertube(inputs, model, n)\n",
        "        print(batch_idx)\n",
        "        if batch_idx < subset_size:\n",
        "            outputs = model(pertubed_inputs)\n",
        "            # Calculate any metrics you need here\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_pert += labels.size(0)\n",
        "            correct_pert += (predicted == labels).sum().item()\n",
        "        else:\n",
        "            break \n",
        "    \n",
        "    accuracy_pert = correct_pert / total_pert\n",
        "    print('Validation Accuracy for {} with pertubed data, {} iterations: {}\\%'.format(name, n, accuracy_pert*100))\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "interpreter": {
      "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
    },
    "kernelspec": {
      "display_name": "Python 3.7.16 64-bit ('venv': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}