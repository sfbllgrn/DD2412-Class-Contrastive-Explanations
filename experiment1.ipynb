{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfbllgrn/DD2412_Class_Contrastive_Explanations/blob/main/experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyJVkIHJo4fY"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sfbllgrn/DD2412_Class_Contrastive_Explanations.git\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_folder = '/content/DD2412_Class_Contrastive_Explanations'\n",
        "destination_folder = '/content'\n",
        "\n",
        "# List the files and subdirectories in the source folder\n",
        "contents = os.listdir(source_folder)\n",
        "\n",
        "# Move each item from the source folder to the destination folder\n",
        "for item in contents:\n",
        "    source_path = os.path.join(source_folder, item)\n",
        "    destination_path = os.path.join(destination_folder, item)\n",
        "    shutil.move(source_path, destination_path)\n",
        "\n",
        "# Remove the now-empty source folder\n",
        "os.rmdir(source_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive that contains all data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xvJHkbHtP-X4",
        "outputId": "0d939e6f-2b77-4edd-c14b-2b9214f5efab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_c_lwZYNjSAo"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torchvision.models import densenet161, DenseNet161_Weights\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "from torchvision.models import alexnet, AlexNet_Weights\n",
        "from torchvision.models import googlenet, GoogLeNet_Weights\n",
        "from torchvision.models import mnasnet0_5, MNASNet0_5_Weights # Här gissar jag att dom använder 0.5, står inte någonstans\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd.functional import jacobian as J\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from torch.nn.functional import one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "Dcrp-A_iovI6",
        "outputId": "e3a23f2c-16b0-4bb6-fe28-ca1de15f55a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bo6tlZJkYMbA"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "\n",
        "# when data is stored locally\n",
        "#data_folder = \"/Users/sofia/Documents/Skola/KTH/Master/Deep Learning, Advanced Course DD2412/Class Contrastive Explanations/DD2412_Class_Contrastive_Explanations/Data_small\"\n",
        "\n",
        "# for data stored on google drive\n",
        "data_folder = \"/content/drive/MyDrive/Colab Notebooks/Deep learning advanced/ImageNet_Data/val\"\n",
        "data_obj = ImageFolder(root=data_folder, transform=DenseNet161_Weights.DEFAULT.transforms())\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "val_dataloader = DataLoader(data_obj, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xp-QZ7E5P4jp"
      },
      "outputs": [],
      "source": [
        "# Init Pretrained models\n",
        "\n",
        "# debug:\n",
        "alex = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
        "pretrained_models = {\"alexnet\":alex}\n",
        "\n",
        "#densenet = densenet161(weights=DenseNet161_Weights.IMAGENET1K_V1)\n",
        "#mobilenet_small = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "#alex = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
        "#google = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "#mnasnet = mnasnet0_5(weights=MNASNet0_5_Weights.IMAGENET1K_V1)\n",
        "#resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "#mobilenet_large = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)  # Denna har även IMAGENET1K_v2\n",
        "#efficientnet = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V1) # Denna har även IMAGENET1K_v2\n",
        "#pretrained_models = {\n",
        " #                    \"alexnet\":alex, \"googlenet\":google,\n",
        " #                    \"mnasnet\":mnasnet, \"resnet\":resnet,\n",
        " #                    \"mobilenet_large\":mobilenet_large,\n",
        " #                    \"efficientnet\":efficientnet,\n",
        " #                    \"densenet\":densenet, \"mobilenet_small\":mobilenet_small}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rIKc699zb1uz"
      },
      "outputs": [],
      "source": [
        "# Perform gradient sign pertubations\n",
        "\n",
        "def calculate_weighted_contrast(x, t, net):\n",
        "  \"\"\"Calculated weighted attribute explanation.\n",
        "  Inputs:\n",
        "    - X: one tensor input data (image), with shape (1,3,224,224)\n",
        "    - t: one-dimensional tensor containing target class for the input img\n",
        "    - net: network model\"\"\"\n",
        "\n",
        "  logits = net(x)\n",
        "  num_classes = logits.shape[1]\n",
        "  phi_t = calculate_gradient(x, t, net)\n",
        "\n",
        "  weighted_explanation = phi_t\n",
        "  alpha_denominator = torch.sum(torch.exp(logits), dim=1)-torch.exp(logits[0,t])\n",
        "  check_alpha_sum = 0\n",
        "  for s in range(num_classes):\n",
        "    if s!=t:\n",
        "      alpha_s = torch.exp(logits[0,s])/alpha_denominator\n",
        "      check_alpha_sum += alpha_s  # this can be checked if it sums to 1, for debug\n",
        "      s = torch.tensor([s]).to(device)\n",
        "      phi_s = calculate_gradient(x, s, net)\n",
        "      weighted_explanation -= alpha_s*phi_s\n",
        "\n",
        "  return weighted_explanation\n",
        "\n",
        "\n",
        "def calculate_mean_contrast(x, t, net):\n",
        "    logits = net(x)\n",
        "    num_classes = logits.shape[1]\n",
        "    phi_t = calculate_gradient(x,t,net)\n",
        "    mean_contrast = phi_t\n",
        "    for s in range(num_classes):\n",
        "      if s!=t:\n",
        "        s = torch.tensor([s]).to(device)    # put the index s in a tensor of the same type/shape as target tensor t\n",
        "        phi_s = calculate_gradient(x,s,net)\n",
        "        mean_contrast -= phi_s/(num_classes-1)\n",
        "\n",
        "    return mean_contrast\n",
        "\n",
        "\n",
        "def calculate_max_contrast(x, t, net):\n",
        "  logits = net(x)\n",
        "  num_classes = logits.shape[1]\n",
        "  logits[0,t] = -10000000000  # because we want to take argmax on logits except t. I assume that there will always be a logit larger than this value\n",
        "  s_star = torch.argmax(logits, dim=1)\n",
        "  phi_s_star = calculate_gradient(x, s_star, net)\n",
        "  phi_t = calculate_gradient(x, t, net)\n",
        "  return phi_t - phi_s_star\n",
        "\n",
        "\n",
        "def get_attribute_explanation(x, t, net, contrast_type):\n",
        "  if contrast_type==\"original\":\n",
        "    return calculate_gradient(x, t, net)\n",
        "  elif contrast_type==\"weighted\":\n",
        "    return calculate_weighted_contrast(x, t, net)\n",
        "  elif contrast_type==\"mean\":\n",
        "    return calculate_mean_contrast(x, t, net)\n",
        "  elif contrast_type==\"max\":\n",
        "    return calculate_max_contrast(x, t, net)\n",
        "\n",
        "\n",
        "def calculate_gradient(x, t, net, probs=False):\n",
        "  logits = net(x)\n",
        "  pred_probab = nn.Softmax(dim=1)(logits)\n",
        "  yt_oh = one_hot(t, num_classes=logits.shape[1])\n",
        "  test = one_hot(torch.tensor(4).to(device), num_classes=logits.shape[1])\n",
        "  external_grad = torch.reshape(yt_oh, logits.shape)\n",
        "\n",
        "  x.grad = None\n",
        "  if probs:\n",
        "    pred_probab.backward(gradient=external_grad)\n",
        "    return x.grad\n",
        "\n",
        "  logits.backward(gradient=external_grad)\n",
        "  return x.grad\n",
        "\n",
        "\n",
        "def calculate_gradient_old(net, x, pred_indx):\n",
        "  value_logits = J(lambda x:net(x)[np.arange(BATCH_SIZE), pred_indx],x)\n",
        "  value_logits = torch.diagonal(value_logits)\n",
        "  value_logits = value_logits.permute(3,0,1,2)\n",
        "  return value_logits\n",
        "\n",
        "\n",
        "def gradient_sign_pertube(x, t, net, N, contrast_type=\"original\"):\n",
        "  epsilon = 1e-3\n",
        "  xn = x.clone()\n",
        "  saved_iterations = []\n",
        "  for n in range(1,N+1):\n",
        "    # I artikeln madry et al. adversarial attacks har dom originaldata här nedan\n",
        "    alpha = epsilon/n\n",
        "    xn = xn + alpha*torch.sign(get_attribute_explanation(x, t, net, contrast_type))\n",
        "    xn = torch.clamp(xn, min=torch.minimum(x-epsilon, torch.tensor(0)), max=torch.maximum(x+epsilon, torch.tensor(1)))\n",
        "\n",
        "    if n in [1,2,10]:\n",
        "      saved_iterations.append(xn.clone())\n",
        "  return saved_iterations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUE29iY0P4jx"
      },
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "C7Yo03J7P4j3",
        "outputId": "fa6785d0-381e-4917-829f-6902e724d1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-12858d61617e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mperturbation_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperturbation_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# perturbed x with respect to logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mx_perturbed_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_sign_pertube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturbation_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_nr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-e5ba6a5f1e92>\u001b[0m in \u001b[0;36mgradient_sign_pertube\u001b[0;34m(x, t, net, N, contrast_type)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# I artikeln madry et al. adversarial attacks har dom originaldata här nedan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_attribute_explanation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrast_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-e5ba6a5f1e92>\u001b[0m in \u001b[0;36mget_attribute_explanation\u001b[0;34m(x, t, net, contrast_type)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcalculate_weighted_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mcontrast_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcalculate_mean_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mcontrast_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcalculate_max_contrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-e5ba6a5f1e92>\u001b[0m in \u001b[0;36mcalculate_mean_contrast\u001b[0;34m(x, t, net)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# put the index s in a tensor of the same type/shape as target tensor t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mphi_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmean_contrast\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mphi_s\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "N=10\n",
        "\n",
        "model_names = [\"alexnet\"]\n",
        "\n",
        "for name in model_names:\n",
        "  model = pretrained_models[name].to(device)\n",
        "  model.eval()\n",
        "\n",
        "  #eval_size = len(data_obj)  # eval on all data\n",
        "  eval_size = 5             # eval on small subset, for debug\n",
        "\n",
        "  perturbation_types = [\"weighted\",\"mean\", \"original\", \"max\"]\n",
        "  accuracy_dict = {i:{p_type:0 for p_type in perturbation_types} for i in [1,2,10]}\n",
        "  accuracy_unperturbed = 0\n",
        "  perturbation_changes = {i:{key:{\"yt\":[], \"pt\":[]} for key in perturbation_types} for i in [1,2,10]}\n",
        "\n",
        "  for batch_idx, (input, target) in enumerate(val_dataloader):\n",
        "      #if batch_idx%eval_size/10 == 0:\n",
        "      print(batch_idx/eval_size)\n",
        "\n",
        "      if batch_idx < eval_size:\n",
        "\n",
        "          input = input.to(device)\n",
        "          input.requires_grad_(True)\n",
        "          target = target.to(device)\n",
        "\n",
        "          y = model(input)\n",
        "          yt = y[np.arange(BATCH_SIZE), target]\n",
        "          p = torch.nn.functional.softmax(y, dim=1)\n",
        "          pt = p[np.arange(BATCH_SIZE), target]\n",
        "\n",
        "          # calculate accuracy for unperturbed data\n",
        "          prediction_unperturbed = torch.argmax(p, dim=1)\n",
        "          if prediction_unperturbed == target:\n",
        "            accuracy_unperturbed += 1/eval_size\n",
        "\n",
        "          for perturbation_type in perturbation_types:\n",
        "            # perturbed x with respect to logits\n",
        "            x_perturbed_list = gradient_sign_pertube(input, target, model, N, perturbation_type)\n",
        "\n",
        "            for i,iter_nr in enumerate([1,2,10]):\n",
        "              x_perturbed = x_perturbed_list[i]\n",
        "              y_perturbed = model(x_perturbed)\n",
        "              yt_perturbed = y_perturbed[np.arange(BATCH_SIZE), target]\n",
        "\n",
        "              # Save the change in yt before and after perturbation\n",
        "              perturbation_changes[iter_nr][perturbation_type]['yt'].append(yt_perturbed-yt)\n",
        "\n",
        "              # Calculate and save change in pt before and after perturbation\n",
        "              pt_perturbed = torch.nn.functional.softmax(y_perturbed, dim=1)[np.arange(BATCH_SIZE), target]\n",
        "              perturbation_changes[iter_nr][perturbation_type]['pt'].append(pt_perturbed-pt)\n",
        "\n",
        "              # Store result of perturbed prediction\n",
        "              _, prediction_perturbed = torch.max(y_perturbed, 1)\n",
        "              if prediction_perturbed == target:\n",
        "                accuracy_dict[iter_nr][perturbation_type] += 1/eval_size\n",
        "      else:\n",
        "          break\n",
        "\n",
        "\n",
        "print(\"true accuracy\", accuracy_unperturbed)\n",
        "for perturbation_type in perturbation_types:\n",
        "  for iter_nr in [1,2,10]:\n",
        "    avg_pt_change = torch.mean(torch.stack(perturbation_changes[iter_nr][perturbation_type]['pt']))\n",
        "    avg_yt_change = torch.mean(torch.stack(perturbation_changes[iter_nr][perturbation_type]['yt']))\n",
        "    accuracy_change = accuracy_dict[iter_nr][perturbation_type]-accuracy_unperturbed\n",
        "    print(\"for n={} and perturbation_type={}:\".format(iter_nr, perturbation_type))\n",
        "    print(\"Average changes in yt: {} and pt: {} and accuracy: {} \\n\".format(avg_yt_change, avg_pt_change, accuracy_change))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# rearrange change_dictionary to desired format:\n",
        "measurements = {\"yt\":tuple([perturbation_changes[p_type]['yt'] for p_type in perturbation_types]),\n",
        "                \"pt\":tuple([perturbation_changes[p_type]['pt'] for p_type in perturbation_types]),\n",
        "                \"acc\":tuple([accuracy_change[p_type] for p_type in perturbation_types])\n",
        "                }\n",
        "\n",
        "\n",
        "print(measurements)\n",
        "fig, axs = plt.subplots(len(model_names), layout=\"constrained\")\n",
        "\n",
        "model_name = \"alexnet\"\n",
        "#\n",
        "#for i,model_name in enumerate(model_names):\n",
        "#  axs[i].set_title(model_name)\n",
        "axs.set_title(model_name)\n",
        "width = 0.25\n",
        "multiplier = 0\n",
        "x = np.arange(len(perturbation_types))\n",
        "\n",
        "for attribute, measurement in measurements.items():\n",
        "    print(measurement)\n",
        "    offset = width * multiplier\n",
        "    rects = axs.bar(x + offset, measurement, width, label=attribute)\n",
        "    axs.bar_label(rects, padding=3)\n",
        "    multiplier += 1\n",
        "\n",
        "\n",
        "x_tickes = (p_type for p_type in perturbation_types)\n",
        "axs.set_xticks(x + width, x_tickes)\n",
        "axs.legend(loc='upper left', ncols=3)\n",
        "axs.set_ylim(0, 250)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yL6AUjAhc-8O",
        "outputId": "313e6c1b-78ac-494b-a16d-e16413cde105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2e7117d4572d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m measurements = {\"yt\":tuple([perturbation_changes[p_type]['yt'] for p_type in perturbation_types]),\n\u001b[1;32m      8\u001b[0m                 \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperturbation_changes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperturbation_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \"acc\": tuple([accuracy_change[p_type] for p_type in perturbation_types])}\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-2e7117d4572d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m measurements = {\"yt\":tuple([perturbation_changes[p_type]['yt'] for p_type in perturbation_types]),\n\u001b[1;32m      8\u001b[0m                 \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperturbation_changes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mperturbation_types\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \"acc\": tuple([accuracy_change[p_type] for p_type in perturbation_types])}\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}