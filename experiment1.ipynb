{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfbllgrn/DD2412_Class_Contrastive_Explanations/blob/main/experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyJVkIHJo4fY"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sfbllgrn/DD2412_Class_Contrastive_Explanations.git\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define the source and destination paths\n",
        "source_folder = '/content/DD2412_Class_Contrastive_Explanations'\n",
        "destination_folder = '/content'\n",
        "\n",
        "# List the files and subdirectories in the source folder\n",
        "contents = os.listdir(source_folder)\n",
        "\n",
        "# Move each item from the source folder to the destination folder\n",
        "for item in contents:\n",
        "    source_path = os.path.join(source_folder, item)\n",
        "    destination_path = os.path.join(destination_folder, item)\n",
        "    shutil.move(source_path, destination_path)\n",
        "\n",
        "# Remove the now-empty source folder\n",
        "os.rmdir(source_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google drive that contains all data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xvJHkbHtP-X4",
        "outputId": "6983bc64-7710-4bc5-a25e-7748fea504d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "_c_lwZYNjSAo"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from torchvision.models import densenet161, DenseNet161_Weights\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "from torchvision.models import alexnet, AlexNet_Weights\n",
        "from torchvision.models import googlenet, GoogLeNet_Weights\n",
        "from torchvision.models import mnasnet0_5, MNASNet0_5_Weights # Här gissar jag att dom använder 0.5, står inte någonstans\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
        "from torchvision.models import efficientnet_b1, EfficientNet_B1_Weights\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd.functional import jacobian as J\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "from torch.nn.functional import one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "Dcrp-A_iovI6",
        "outputId": "608f8185-aa96-402e-d19a-fb88fa280f36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Bo6tlZJkYMbA"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "\n",
        "# when data is local\n",
        "#data_folder = \"/Users/sofia/Documents/Skola/KTH/Master/Deep Learning, Advanced Course DD2412/Class Contrastive Explanations/DD2412_Class_Contrastive_Explanations/Data_small\"\n",
        "\n",
        "# data on google drive\n",
        "data_folder = \"/content/drive/MyDrive/Colab Notebooks/Deep learning advanced/ImageNet_Data/val\"\n",
        "data_obj = ImageFolder(root=data_folder, transform=DenseNet161_Weights.DEFAULT.transforms())\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "val_dataloader = DataLoader(data_obj, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "xp-QZ7E5P4jp"
      },
      "outputs": [],
      "source": [
        "# Init Pretrained models\n",
        "\n",
        "densenet = densenet161(weights=DenseNet161_Weights.IMAGENET1K_V1)\n",
        "mobilenet_small = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)\n",
        "alex = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
        "google = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "mnasnet = mnasnet0_5(weights=MNASNet0_5_Weights.IMAGENET1K_V1)\n",
        "resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "mobilenet_large = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)  # Denna har även IMAGENET1K_v2\n",
        "efficientnet = efficientnet_b1(weights=EfficientNet_B1_Weights.IMAGENET1K_V1) # Denna har även IMAGENET1K_v2\n",
        "pretrained_models = {\n",
        "                     \"alexnet\":alex, \"googlenet\":google,\n",
        "                     \"mnasnet\":mnasnet, \"resnet\":resnet,\n",
        "                     \"mobilenet_large\":mobilenet_large,\n",
        "                     \"efficientnet\":efficientnet,\n",
        "                     \"densenet\":densenet, \"mobilenet_small\":mobilenet_small}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "rIKc699zb1uz"
      },
      "outputs": [],
      "source": [
        "# Perform gradient sign pertubations\n",
        "\n",
        "def backward_gradient_explanation(X, label, net, probs=False):\n",
        "  logits = net(X)\n",
        "  pred_probab = nn.Softmax(dim=1)(logits)\n",
        "  yt_oh = one_hot(label, num_classes=logits.shape[1])\n",
        "  external_grad = torch.reshape(yt_oh, logits.shape)\n",
        "  X.retain_grad()\n",
        "  if probs:\n",
        "    pred_probab.backward(gradient=external_grad)\n",
        "    return X.grad\n",
        "\n",
        "  logits.backward(gradient=external_grad)\n",
        "\n",
        "  return X.grad\n",
        "\n",
        "def attribution_explanation(net, x, pred_indx):\n",
        "  value_logits = J(lambda x:net(x)[np.arange(BATCH_SIZE), pred_indx],x)\n",
        "  value_logits = torch.diagonal(value_logits)\n",
        "  value_logits = value_logits.permute(3,0,1,2)\n",
        "  #value_probs = J(lambda x:torch.nn.functional.softmax(net(x), dim=1)[np.arange(BATCH_SIZE),pred_indx], x)\n",
        "  #value_probs = torch.diagonal(value_probs)\n",
        "  #value_probs = value_probs.permute(3,0,1,2)\n",
        "  return value_logits\n",
        "\n",
        "\n",
        "def gradient_sign_pertube(inputs, labels, net, n):\n",
        "  epsilon = 1e-3\n",
        "  x_logits = inputs.clone()\n",
        "  alpha = epsilon/n\n",
        "  for i in range(n):\n",
        "    # Att tänka ut: ska det vara x eller data nedan\n",
        "    logits = backward_gradient_explanation(inputs, labels, net, probs=False)\n",
        "    x_logits = x_logits + alpha*torch.sign(logits)  # blir det rätt index här?\n",
        "    x_logits = torch.clamp(x_logits, min=torch.minimum(inputs-epsilon, torch.tensor(0)), max=torch.maximum(inputs+epsilon, torch.tensor(1)))\n",
        "\n",
        "  return x_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUE29iY0P4jx"
      },
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "C7Yo03J7P4j3",
        "outputId": "257c3013-2e7a-4e9a-8537-a23b8f5a5cb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-b006eb05abc0>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mperturbed_x_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_sign_pertube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0msubset_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-4ffe780bab54>\u001b[0m in \u001b[0;36mgradient_sign_pertube\u001b[0;34m(inputs, labels, net, n)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Att tänka ut: ska det vara x eller data nedan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_gradient_explanation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mx_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_logits\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blir det rätt index här?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mx_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-126-4ffe780bab54>\u001b[0m in \u001b[0;36mbackward_gradient_explanation\u001b[0;34m(X, label, net, probs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexternal_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 144.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 136.81 MiB is free. Process 43799 has 14.61 GiB memory in use. Of the allocated memory 13.47 GiB is allocated by PyTorch, and 55.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "n=1\n",
        "\n",
        "#for name, model in pretrained_models.items():\n",
        "\n",
        "name = \"alex\"\n",
        "model = alex.to(device)\n",
        "\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "\n",
        "subset_size = len(data_obj)\n",
        "subset_size = 100\n",
        "\n",
        "correct = {\"probs\":0, \"logits\":0, \"unperturbed\":0}\n",
        "total = {\"probs\":0, \"logits\":0, \"unperturbed\":0}\n",
        "changes = {\"yt\":[], \"pt\":[]}\n",
        "\n",
        "#with torch.no_grad():\n",
        "correct_pert = {\"probs\":0, \"logits\":0, \"unperturbed\":0}\n",
        "total_pert = 0\n",
        "for batch_idx, (inputs, labels) in enumerate(val_dataloader):\n",
        "    inputs.requires_grad_(True)\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    perturbed_x_logits = gradient_sign_pertube(inputs, labels, model, n)\n",
        "\n",
        "    if batch_idx%subset_size/10 == 0:\n",
        "      print(batch_idx/subset_size)\n",
        "\n",
        "    if batch_idx < subset_size:\n",
        "        y_pert = model(perturbed_x_logits)\n",
        "        y = model(inputs)\n",
        "\n",
        "        yt_pert = y_pert[np.arange(BATCH_SIZE), labels]\n",
        "        yt = y[np.arange(BATCH_SIZE), labels]\n",
        "\n",
        "        changes['yt'].append(yt_pert-yt)\n",
        "\n",
        "        pt_pert = torch.nn.functional.softmax(y_pert, dim=1)[np.arange(BATCH_SIZE), labels]\n",
        "        pt = torch.nn.functional.softmax(y, dim=1)[np.arange(BATCH_SIZE), labels]\n",
        "        changes['pt'].append(pt_pert-pt)\n",
        "\n",
        "\n",
        "        #_, predicted_probs = torch.max(outputs_probs, 1)\n",
        "        _, predictions_pert = torch.max(y_pert, 1)\n",
        "        _, predictions_unperturbed = torch.max(y, 1)\n",
        "        total_pert += labels.size(0)\n",
        "        correct_pert[\"logits\"] += (predictions_pert == labels).sum().item()\n",
        "        #correct_pert[\"probs\"] += (predicted_probs == labels).sum().item()\n",
        "        correct_pert[\"unperturbed\"] += (predictions_unperturbed == labels).sum().item()\n",
        "\n",
        "    else:\n",
        "        break\n",
        "\n",
        "accuracy_logits = correct_pert['logits']/ total_pert\n",
        "#accuracy_probs = correct_pert['probs']/ total_pert\n",
        "accuracy_unperturbed = correct_pert['unperturbed']/ total_pert\n",
        "\n",
        "print(\"Average changes in yt: {} and pt: {}\".format(torch.mean(torch.stack(changes['yt'])), torch.mean(torch.stack(changes['pt']))))\n",
        "print('Validation Accuracy for {} with pertubed data on logits, {} iterations: {}\\%'.format(name, n, accuracy_logits*100))\n",
        "#print('Validation Accuracy for {} with pertubed data on probs, {} iterations: {}\\%'.format(name, n, accuracy_probs*100))\n",
        "print('Validation Accuracy for {} with pertubed data on unperturbed data, {} iterations: {}\\%'.format(name, n, accuracy_unperturbed*100))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yL6AUjAhc-8O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "b2f5f294937e1f47dd6e010afb2ca0c96836afcb29d9a31a278c78890f03e991"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}